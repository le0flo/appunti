\documentclass{article}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}

\begin{document}

\section{Legge dei grandi numeri}

\subsection{Introduzione}

Sia $X$ la variabile casuale che rappresenta l'esito di un esperimento.
$E[X]$ sarà la grandezza fisica da misurare, $Var(X)$ è proporzionale all'errore di misura al quadrato, perciò $\sqrt{Var(X)}$ è proporzionale all'errore di misura.
Se l'esperimento viene ripetuto $N$ volte, si introducono $X_1, \dots, X_N$ per rappresentare gli esiti di ciascun esperimento.
$X_1, \dots, X_N$ sono variabili casuali indipendenti e identicamente distribuite (v.c.i.i.d.), nel caso in cui l'esperimento venga ripetuto in maniera indipendente e identica.

\noindent
$\bar{X} = \frac{\sum^N_{k=1}X_k}{N}$ è la media aritmetica, o media campionaria in inferenza statistica.
$\bar{X}$ è una variabile casuale, funzione di $X_1, \dots, X_N$.
Se $X_1, \dots, X_N$ sono identicamente distribuite, $\mu=E[X_1]=\dots=E[X_N]$ e $\sigma^2=Var(X_1)=\dots=Var(X_N)$.

$$
\begin{matrix}
E[X] = E[\frac{\sum^N_{k=1}X_k}{N}] = \frac{1}{N} E[\sum^N_{k=1}X_k] = \frac{1}{N} \sum^N_{k=1} E[X_k] = \frac{1}{N} \sum^N_{k=1} \mu = \frac{N\mu}{N} &= \mu \\
Var(X) = Var(\frac{\sum^N_{k=1}X_k}{N}) = \frac{1}{N^2} Var(\sum^N_{k=1}X_k) = \frac{1}{N^2} \sum^N_{k=1} Var(X_k) = \frac{1}{N^2} \sum^N_{k=1} \sigma^2 = \frac{N\sigma^2}{N^2} &= \frac{\sigma^2}{N}
\end{matrix}
$$

\noindent
Se $N$ tende ad infinito, il valore medio rimane invariato, mentre l'errore tende a 0. Questo è il concetto dietro la legge dei grandi numeri.

\subsection{Disuguaglianza di Markov}

Sia $X$ una variabile casuale con valori $\mathbb{R}^+ \backslash \{0\}$, con $E[X]=\mu$ e sia $a \in \mathbb{R}^+$.

$$
P(X \geq a) \leq \frac{E[X]}{a}
$$

\noindent
Questa disuguaglianza è sempre vera nella teoria, ma nella pratica risulta utile soltanto se $a > E[X]$.

\subsection{Disuguaglianza di Cebycev}

Data una variabile casuale $X$ con $E[X]=\mu$, $Var(X)=\sigma^2$ e considerata $r \in \mathbb{R}^+$.

$$
P(|X - \mu| \geq r) \leq \frac{\sigma^2}{r^2}
$$

\noindent
Questa disuguaglianza viene impiegata come stima della probabilità soltanto se $\frac{\sigma^2}{r^2} < 1$.

\subsection{Legge dei grandi numeri (formulazione debole)}

Sia data una successione di variabili casuali indipendenti e identicamente distribuite (v.c.i.i.d.) $X_1, \dots, X_N$ con $E[X_k]=\mu$, $Var(X_k)=\sigma^2$ e $k = 1,\dots,N$.
Allora per ogni $\varepsilon > 0$ piccolo a piacere:

$$
\lim_{N\to+\infty}P(|\frac{\sum^N_{k=1}X_k}{N} - \mu| > \varepsilon) = 0
$$

\noindent
Ovvero, la media aritmetica delle $X_k$ converge in probabilità alla media teoria $\mu$.

\end{document}
